{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d827a92-d03c-4e5e-8dbd-92bd9f7f5129",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# paramter cell do not remove!!\n",
    "nb_parm=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575c082c-c36d-49c2-a58f-f477e2635af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"/home/jovyan/notebooks\")\n",
    "from Framework.module import Utility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3948dd-3761-4bdd-b62c-e8eb094ea50b",
   "metadata": {},
   "source": [
    "## Do the task After this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6d7aea-d442-400e-9dd3-104a3c7597f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019d45a1-09e3-4095-bcb2-dc0b3fce5c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "libraries = [\n",
    "    'pyspark', 'findspark', 'boto3', 'papermill', 'pyarrow', 'pandas', \n",
    "    'langchain', 'ollama', 'langchain_ollama', 'langchain_chroma'\n",
    "]\n",
    "\n",
    "for library in libraries:\n",
    "    try:\n",
    "        exec(f\"import {library}\")\n",
    "        print(f\"{library} is installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"{library} is NOT installed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb50f66b-425e-4436-a0e1-622dd2a1a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Since Ollama and Jupyter Notebook run in separate containers, \n",
    "Jupyter needs to communicate with Ollama via HTTP requests, \n",
    "because they are not in the same process or environment.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1910727-4204-4506-b629-83412dbf89ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "The capital of Thailand is Bangkok.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "# Explicitly define Ollama's API base URL\n",
    "llm = OllamaLLM(\n",
    "    model=\"deepseek-r1:7b\",\n",
    "    temperature=0,\n",
    "    # base_url=\"http://ollama:11434\"\n",
    "    base_url=\"http://host.docker.internal:11434\" # need to refer because it run as separate container the langchain not know what the end point of model are so this is need to be attached\n",
    "\n",
    ")\n",
    "\n",
    "# Define a simple question prompt\n",
    "question = \"What is the capital of thailand?\"\n",
    "response = llm.invoke(question)\n",
    "\n",
    "# Print the response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b14a9a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/asyncio/selector_events.py:864: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=65 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/opt/conda/lib/python3.11/asyncio/selector_events.py:864: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=58 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'WeaviateClient' object has no attribute 'schema'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 38\u001b[0m\n\u001b[1;32m     18\u001b[0m schema \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclasses\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m     20\u001b[0m         {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     ]\n\u001b[1;32m     35\u001b[0m }\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Check if the schema contains the 'Person' class using the correct method\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerson\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[38;5;241m.\u001b[39mget():\n\u001b[1;32m     39\u001b[0m     client\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mcreate(schema)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Now let's add a person object to the database\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'WeaviateClient' object has no attribute 'schema'"
     ]
    }
   ],
   "source": [
    "# import weaviate\n",
    "\n",
    "# # Establish a connection to the Weaviate instance\n",
    "# client = weaviate.connect_to_custom(\n",
    "#     http_host=\"host.docker.internal\",\n",
    "#     http_port=8082,\n",
    "#     http_secure=False,  # Set to True if using HTTPS\n",
    "#     grpc_host=\"host.docker.internal\",\n",
    "#     grpc_port=50051,\n",
    "#     grpc_secure=False,  # Set to True if using a secure gRPC connection\n",
    "# )\n",
    "\n",
    "# # Check if the client is ready\n",
    "# print(client.is_ready())  # Should return True if the connection is successful\n",
    "\n",
    "# # Define the schema for the object (if you haven't already defined it)\n",
    "# # Example: Create a simple schema for 'Person' with 'name' and 'age' properties\n",
    "# schema = {\n",
    "#     \"classes\": [\n",
    "#         {\n",
    "#             \"class\": \"Person\",\n",
    "#             \"description\": \"A class to represent people\",\n",
    "#             \"properties\": [\n",
    "#                 {\n",
    "#                     \"name\": \"name\",\n",
    "#                     \"dataType\": [\"text\"]\n",
    "#                 },\n",
    "#                 {\n",
    "#                     \"name\": \"age\",\n",
    "#                     \"dataType\": [\"int\"]\n",
    "#                 }\n",
    "#             ]\n",
    "#         }\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# # Check if the schema contains the 'Person' class using the correct method\n",
    "# if \"Person\" not in client.schema.get():\n",
    "#     client.schema.create(schema)\n",
    "\n",
    "# # Now let's add a person object to the database\n",
    "# person_data = {\n",
    "#     \"name\": \"John Doe\",\n",
    "#     \"age\": 30\n",
    "# }\n",
    "\n",
    "# # Write the object to Weaviate\n",
    "# client.data_object.create(\n",
    "#     data_object=person_data,\n",
    "#     class_name=\"Person\"\n",
    "# )\n",
    "\n",
    "# # Now let's list the objects stored in the \"Person\" class\n",
    "# results = client.data_object.get(class_name=\"Person\")\n",
    "\n",
    "# # Print out the stored objects\n",
    "# for obj in results['data']['Get']['Person']:\n",
    "#     print(f\"Name: {obj['name']}, Age: {obj['age']}\")\n",
    "\n",
    "# # Close the connection\n",
    "# client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c7c9bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_97/2360585613.py:29: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embedding=OllamaEmbeddings(\n",
      "/opt/conda/lib/python3.11/site-packages/langsmith/client.py:278: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<think>\\nOkay, so I need to figure out what Task Decomposition is based on the given context. Let me read through the context carefully.\\n\\nThe context mentions that complicated tasks are broken down into smaller steps by an agent using Chain of Thought (CoT). It says CoT helps models handle complex tasks by thinking step-by-step, making it easier for them to use more computation time and understand their thought process better. Task decomposition is further explained with examples like using simple prompting or task-specific instructions.\\n\\nLooking at the figure mentioned, Fig. 1 shows an overview of a LLM-powered autonomous agent system where planning is involved before executing tasks. Then there's another part about how HuggingGPT works in four stages: task planning, instruction parsing, execution, and logging results.\\n\\nThe Tree of Thoughts (Yao et al. 2023) extends CoT by creating a tree structure of thought steps, allowing for multiple possibilities at each step using BFS or DFS with classifier prompts.\\n\\nSo putting this together, Task Decomposition refers to breaking down complex tasks into smaller, manageable parts. This is done through methods like Chain of Thought prompting, which guides the model to think step-by-step. It can be triggered by simple instructions, task-specific guidelines, or human input. The goal is to make problem-solving more efficient and understandable for both humans and AI systems.\\n</think>\\n\\nTask Decomposition involves breaking down complex tasks into smaller, manageable steps using techniques like Chain of Thought (CoT). This method helps models process intricate problems efficiently by thinking step-by-step, enhancing performance and interpretability. It can be triggered through simple prompts, task-specific instructions, or human guidance, aiding both AI systems and humans in problem-solving.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import bs4\n",
    "# from langchain import hub\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "# from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# #### INDEXING ####\n",
    "\n",
    "# # Load Documents\n",
    "# loader = WebBaseLoader(\n",
    "#     web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "#     bs_kwargs=dict(\n",
    "#         parse_only=bs4.SoupStrainer(\n",
    "#             class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "#         )\n",
    "#     ),\n",
    "# )\n",
    "# docs = loader.load()\n",
    "\n",
    "# # Split\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "# splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# # Embed\n",
    "# vectorstore = Chroma.from_documents(documents=splits, \n",
    "#                                     embedding=OllamaEmbeddings(\n",
    "#                                                 model=\"mxbai-embed-large\",  # Or another embedding model, like `nomic-embed-text`\n",
    "#                                                 base_url=\"http://host.docker.internal:11434\"\n",
    "#                                             )\n",
    "#                                    )\n",
    "\n",
    "# retriever = vectorstore.as_retriever()\n",
    "\n",
    "# #### RETRIEVAL and GENERATION ####\n",
    "\n",
    "# # Prompt\n",
    "# prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# # LLM\n",
    "# llm = llm = OllamaLLM(\n",
    "#     model=\"deepseek-r1:7b\",\n",
    "#     temperature=0,\n",
    "#     base_url=\"http://host.docker.internal:11434\" \n",
    "# )\n",
    "\n",
    "# # Post-processing\n",
    "# def format_docs(docs):\n",
    "#     return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# # Chain\n",
    "# rag_chain = (\n",
    "#     {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "#     | prompt\n",
    "#     | llm\n",
    "#     | StrOutputParser()\n",
    "# )\n",
    "\n",
    "# # Question\n",
    "# rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
