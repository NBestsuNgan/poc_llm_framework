{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "304e61e8-c5f5-4269-9468-578331858a2f",
   "metadata": {},
   "source": [
    "### Purpose of this agent is to call a right too for a right task from user query not universal answer any question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25f4e852-e9cf-43d1-b29a-e0a310a6ca9f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# paramter cell do not remove!!\n",
    "# nb_parm='datalake|raw/pdf|Birddiversityanddistribution|pdf||300|150'\n",
    "# nb_parm='datalake|raw/text-csv|PFW_spp_translation_table_May2024|csv||300|150'\n",
    "nb_parm='llmnok'\n",
    "question = 'how many bird species are in migratory?'\n",
    "embed_model = \"mxbai-embed-large\" \n",
    "gen_model = \"deepseek-r1:7b\"\n",
    "# collection = \"Bridknowledge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b058a7b6-4199-43ba-895d-c9c27fe5ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"/home/jovyan/notebooks\")\n",
    "from Framework.module import Utility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b215064d-3405-4b85-92ce-088f54fd98ae",
   "metadata": {},
   "source": [
    "## Do the task after this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc3aa2b-a6ac-46a5-a970-3fa38d1a9976",
   "metadata": {},
   "source": [
    "## Register LLM and bind tools from pool of tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052f60e7-d71d-4d8a-baa2-929ee6a80748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "import importlib\n",
    "import inspect\n",
    "from importlib import import_module\n",
    "from langchain_core.tools import BaseTool\n",
    "from email.message import EmailMessage\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_ollama import ChatOllama\n",
    "import smtplib\n",
    "from Framework import base_tools\n",
    "\n",
    "def load_all_tools(module_path: str):\n",
    "    \"\"\"\n",
    "    Load all @tool-decorated functions from a module.\n",
    "\n",
    "    Args:\n",
    "        module_path (str): Python module path like 'tools'\n",
    "\n",
    "    Returns:\n",
    "        List of tool function objects\n",
    "    \"\"\"\n",
    "    tools = []\n",
    "    module = import_module(module_path)\n",
    "    for name, obj in inspect.getmembers(module):\n",
    "        if isinstance(obj, BaseTool):\n",
    "            tools.append(obj)\n",
    "    return tools\n",
    "    \n",
    "utility_tools = load_all_tools(\"Framework.pool_of_tools\")\n",
    "\n",
    "# qwen2.5:7b\n",
    "# llama3-groq-tool-use:8b\n",
    "llm = ChatOllama(\n",
    "    model=\"qwen2.5:7b\",\n",
    "    temperature=0,\n",
    "    base_url=\"http://host.docker.internal:11434\",\n",
    "    functions=utility_tools,      # pass the list of BaseTool\n",
    "    function_call=\"auto\"          # let the model decide when to call\n",
    ")\n",
    "\n",
    "llm_with_tools = llm.bind_tools(utility_tools)\n",
    "\n",
    "utility_tools\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdbd97c-316b-4ac8-88e1-d9a9459c2fb9",
   "metadata": {},
   "source": [
    "## add retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57612c4f-e220-4ef3-a30a-49d6863a2d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embedding_model = OllamaEmbeddings(\n",
    "    model=\"mxbai-embed-large\",\n",
    "    base_url=\"http://host.docker.internal:11434\"\n",
    ")\n",
    "\n",
    "list_of_web_research = {\n",
    "                        \"https://clevertap.com/blog/rfm-analysis/?utm_source=chatgpt.com\": (\"blogInner__content\"),\n",
    "                        \"https://www.techtarget.com/searchdatamanagement/definition/RFM-analysis?utm_source=chatgpt.com\": (\"content-columns\"),\n",
    "                        \"https://www.mckinsey.com/capabilities/growth-marketing-and-sales/our-insights/the-value-of-getting-personalization-right-or-wrong-is-multiplying?utm_source=chatgpt.com\": (\"mck-o-container--outer\"),\n",
    "                        \"https://www.optimove.com/resources/learning-center/rfm-segmentation?utm_source=chatgpt.com\": (\"single-article\"),\n",
    "                        \"https://business.adobe.com/blog/basics/marketing-personalization?utm_source=chatgpt.com\": (\"content\"),\n",
    "                        \"https://www.adquadrant.com/increasing-engagement-with-behavioral-transactional-and-predictive-data-in-lifecycle-marketing/?utm_source=chatgpt.com\": (\"article w-richtext\"),\n",
    "                        \"https://www.actioniq.com/blog/what-is-rfm-analysis/?utm_source=chatgpt.com\": (\"fl-module fl-module-fl-post-content fl-node-8mnrcgqytwf4\"),\n",
    "                        \"https://medium.com/%40yennhi95zz/using-rfm-analysis-for-effective-customer-segmentation-in-marketing-4964a99bf606?utm_source=chatgpt.com\": (\"cj bh gb gc gd ge\")\n",
    "                       }\n",
    "\n",
    "for web, classes in list_of_web_research.items():\n",
    "    try:\n",
    "        loader = WebBaseLoader(\n",
    "            web_paths=(web,),\n",
    "            bs_kwargs=dict(\n",
    "                parse_only=bs4.SoupStrainer(\n",
    "                    class_=classes\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "        docs = loader.load()\n",
    "        print(web, docs)\n",
    "        # Split\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        splits = text_splitter.split_documents(docs)\n",
    "        \n",
    "        # Embed\n",
    "        vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                            embedding=embedding_model)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b1f7e3-f137-4983-a968-2bbf2bb605e8",
   "metadata": {},
   "source": [
    "## Define ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eba3984-ed39-45da-8330-c029108bd734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/chromadb/types.py:144: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  return self.model_fields  # pydantic 2.x\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import MessagesState # holds the conversation messages (LLM, human, tools).\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage # Different types of messages in the conversation.\n",
    "from typing_extensions import Literal, TypedDict  # Typed return values for conditional logic.\n",
    "from langgraph.graph import StateGraph, START, END # Special node constants\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# get knowledge\n",
    "docs = retriever.invoke(\"customer reward personalization strategy\")\n",
    "retrieved_chroma_knowledge = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Nodes\n",
    "def llm_call(state: MessagesState):\n",
    "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            llm_with_tools.invoke(\n",
    "                [\n",
    "                    SystemMessage(\n",
    "                        content=f\"\"\"\n",
    "                    You are an intelligent recommendation system tasked with analyzing customer data and suggesting the most relevant promotions, reward types, and discount credits. Your goal is to provide personalized recommendations based on patterns and historical behavior.\n",
    "                    \n",
    "                    Below is a markdown table containing metadata about all the available tables in the database. Each table includes a comment explaining the nature of the data it contains.\n",
    "                    \n",
    "                    {base_tools.get_table_description()}\n",
    "                    \n",
    "                    You also have access to a knowledge base containing prior marketing strategy guidelines, past customer behavior patterns, and examples of successful campaigns. Use this knowledge to enrich your reasoning and support your recommendations.\n",
    "                    \n",
    "                    Knowledge base snippets:\n",
    "                    {retrieved_chroma_knowledge}\n",
    "                    \n",
    "                    You have access to two helper functions:\n",
    "                    - `describe_table(table_name)` – to explore the schema of a specific table.\n",
    "                    - `execute_sql(query)` – to run SQL queries for more insights based on your judgment.\n",
    "                    \n",
    "                    Use this information to guide your analysis and provide reasoned, justifiable suggestions.\n",
    "                    \"\"\"\n",
    "                    )\n",
    "                ]\n",
    "                + state[\"messages\"]\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def tool_node(state: dict):\n",
    "    \"\"\"Performs the tool call\"\"\"\n",
    "\n",
    "    result = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool = tools_by_name[tool_call[\"name\"]] #Finds the matching tool by name.\n",
    "        observation = tool.invoke(tool_call[\"args\"])\n",
    "        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n",
    "        \n",
    "    # print(\"tool_node\", result)\n",
    "    return {\"messages\": result}\n",
    "\n",
    "  \n",
    "\n",
    "# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\n",
    "def should_continue(state: MessagesState) -> Literal[\"environment\", END]:\n",
    "    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1] #Looks at the most recent AI message\n",
    "    # If the LLM makes a tool call, then perform an action\n",
    "    if last_message.tool_calls:\n",
    "        return \"Action\"\n",
    "    # Otherwise, we stop (reply to the user)\n",
    "    return END\n",
    "\n",
    "\n",
    "# Build workflow\n",
    "agent_builder = StateGraph(MessagesState)\n",
    "\n",
    "# Add nodes\n",
    "agent_builder.add_node(\"llm_call\", llm_call)\n",
    "agent_builder.add_node(\"environment\", tool_node)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "agent_builder.add_edge(START, \"llm_call\")\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm_call\",\n",
    "    should_continue,\n",
    "    {\n",
    "        # Name returned by should_continue : Name of next node to visit\n",
    "        \"Action\": \"environment\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "agent_builder.add_edge(\"environment\", \"llm_call\")\n",
    "\n",
    "# Compile the agent\n",
    "agent = agent_builder.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f6b4f-b83d-4473-8fa1-dd25110cc9a8",
   "metadata": {},
   "source": [
    "## Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b78bfac6-df7e-4b23-8cc5-cba14d5d23a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/notebooks/Framework/base_tools.py:87: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql_query(\"\"\"\n"
     ]
    }
   ],
   "source": [
    "from Framework import base_tools\n",
    "import pandas as pd\n",
    "\n",
    "user_demographic = base_tools.get_user_demographic()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0732abe-b121-4ea0-a8a9-ee9460f01637",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_result = []\n",
    "previous_user_info = []\n",
    "display(Image(agent.get_graph(xray=True).draw_mermaid_png()))\n",
    "\n",
    "for iterate in range(int(len(user_demographic)/4)):\n",
    "    print(f\"iterate number : {iterate+1}\")\n",
    "    if iterate == 0:\n",
    "        user_info = user_demographic.iloc[iterate*4:(iterate*4)+4, ::].to_markdown()\n",
    "    \n",
    "        instruction = f\"\"\"\n",
    "        You are a recommendation engine trained to analyze customer profiles and suggest personalized marketing strategies.\n",
    "\n",
    "        Here is the current customer's demographic and profile data. Use this to personalize your recommendations:\n",
    "    \n",
    "        {user_info}\n",
    "\n",
    "        Using RFM segmentation (recency, frequency, monetary) based on OVERALL_PERMONTH and OVERALL_PERTXN tables,  combined with demographic filters from the DEMOGRAPHIC table, and category spending insights from MERCHANT_GROUP_TRANSPORTATION, recommend the next best promotion_cr, reward_type, and shopping_discount_cr for this customer.\n",
    "\n",
    "        Based on the data and your analysis, answer the following question:\n",
    "    \n",
    "        **What is the next suggestion for `promotion_cr`, `reward_type`, and `shopping_discount_cr` for this customer?**\n",
    "    \n",
    "        Respond in this format:\n",
    "        - **promotion_cr**: [Your recommendation] - [Justification]\n",
    "        - **reward_type**: [Your recommendation] - [Justification]\n",
    "        - **shopping_discount_cr**: [Your recommendation] - [Justification]\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        messages = [HumanMessage(content= instruction)]\n",
    "        \n",
    "        messages = agent.invoke({\"messages\": messages})\n",
    "        for m in messages[\"messages\"]:                                                                              \n",
    "            m.pretty_print()\n",
    "            \n",
    "        print(f\"prediction result from iterate {iterate + 1}: {messages['messages'][-1]}\")\n",
    "        previous_user_info.append(user_info)  \n",
    "        predicted_result.append(messages[\"messages\"][-1])  \n",
    "\n",
    "    else:\n",
    "        user_info = user_demographic.iloc[iterate*4:(iterate*4)+4, ::].to_markdown()\n",
    "    \n",
    "        last_suggest_prompt = f\"\"\"\n",
    "        You are a recommendation engine trained to analyze customer profiles and suggest personalized marketing strategies.\n",
    "\n",
    "        As context, here is the previously generated recommendation from your last prediction. You may reference it for background, but do not copy or overly rely on it. Instead, use it to better understand the evolution of the customer's behavior and preferences.\n",
    "\n",
    "        Previous customer information:\n",
    "        \n",
    "        {previous_user_info[iterate -1]}\n",
    "        \n",
    "        Previous customer prediction:\n",
    "        \n",
    "        {predicted_result[iterate -1]}\n",
    "        \"\"\"\n",
    "        \n",
    "        instruction = f\"\"\"\n",
    "        Here is the current customer's demographic and profile data. Use this to personalize your recommendations:\n",
    "    \n",
    "        {user_info}\n",
    "\n",
    "        Using RFM segmentation (recency, frequency, monetary) based on OVERALL_PERMONTH and OVERALL_PERTXN tables,  combined with demographic filters from the DEMOGRAPHIC table, and category spending insights from MERCHANT_GROUP_TRANSPORTATION, recommend the next best promotion_cr, reward_type, and shopping_discount_cr for this customer.\n",
    "    \n",
    "        Based on the data and your analysis, answer the following question:\n",
    "    \n",
    "        **What is the next suggestion for `promotion_cr`, `reward_type`, and `shopping_discount_cr` for this customer?**\n",
    "    \n",
    "        Respond in this format:\n",
    "        - **promotion_cr**: [Your recommendation] - [Justification]\n",
    "        - **reward_type**: [Your recommendation] - [Justification]\n",
    "        - **shopping_discount_cr**: [Your recommendation] - [Justification]        \n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        messages = [HumanMessage(content= last_suggest_prompt + instruction)]\n",
    "        \n",
    "        messages = agent.invoke({\"messages\": messages})\n",
    "        for m in messages[\"messages\"]:                                                                              \n",
    "            m.pretty_print()\n",
    "        \n",
    "        print(f\"prediction result from iterate {iterate + 1}: {messages['messages'][-1]}\")\n",
    "        previous_user_info.append(user_info)  \n",
    "        predicted_result.append(messages[\"messages\"][-1])  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de18c727-4fda-4c45-b678-10c04d3c84f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in predicted_result:\n",
    "    print(_.pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21bec12-ea38-4113-8a73-25c3a4f2904d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
