{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f605d5c3-bd08-478e-be5b-d257f82e858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramter cell do not remove!!\n",
    "nb_parm='llmnok'\n",
    "embed_model = \"mxbai-embed-large\" \n",
    "gen_model = \"deepseek-r1:7b\"\n",
    "collection = \"Bridknowledge\"\n",
    "question = \"give me a species_code that have scientific_name equal to Cairina moschata and leatest taxonomic_sort_order\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b61b08b0-2579-4cb5-9992-2f8fbc53618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"/home/jovyan/notebooks\")\n",
    "from Framework.module import Utility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc57bfd6-d5e1-4ffe-bae5-9347f7c69f4f",
   "metadata": {},
   "source": [
    "## Do the task After this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4b4f2efc-77d3-4b16-8df0-67e3f09897ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_parm:  llmnok\n",
      "embed_model:  mxbai-embed-large\n",
      "gen_model:  deepseek-r1:7b\n",
      "collection:  Bridknowledge\n",
      "question:  give me a species_code that have scientific_name equal to Cairina moschata and leatest taxonomic_sort_order\n"
     ]
    }
   ],
   "source": [
    "print(\"nb_parm: \",nb_parm)\n",
    "print(\"embed_model: \",embed_model)\n",
    "print(\"gen_model: \",gen_model)\n",
    "print(\"collection: \",collection)\n",
    "print(\"question: \",question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55c3f53-d08f-4019-a32e-b993e24365c3",
   "metadata": {},
   "source": [
    "## Download module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7821f000-f450-4975-948e-13cc4efe68d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import requests\n",
    "import json\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e74d09-2ada-40d7-a272-d1e15f29c1f5",
   "metadata": {},
   "source": [
    "## Provide highest score in simmilarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3b37d6d5-2b0b-4be2-89c4-99d3d053e09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highest_score_sim(docs):\n",
    "    # Tokenize each document\n",
    "    tokenized_documents = [word_tokenize(page.lower()) for page in docs]\n",
    "    \n",
    "    # Initialize the BM25 model\n",
    "    bm25 = BM25Okapi(tokenized_documents)\n",
    "    \n",
    "    # Tokenize the query\n",
    "    tokenized_query = word_tokenize(question.lower())\n",
    "    \n",
    "    # Get the BM25 scores for each document based on the query\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # Rank the documents by score\n",
    "    return sorted(zip(scores, docs), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a083f83c-6028-4d9e-afd9-706b85635a62",
   "metadata": {},
   "source": [
    "## Vector db retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f8be6b8a-e23a-4ad6-8399-a1c35eeaf481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Title': {'PFW_spp_translation_table_May2024.csv'}, 'Content': 'musduc | nan | 10 | Cairina moschata | Muscovy Duck | 2023 | 423'}\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "import json\n",
    "from datetime import datetime\n",
    "from weaviate.classes.query import MetadataQuery, Filter\n",
    "from weaviate.classes.query import Filter\n",
    "\n",
    "# # search1\n",
    "def search_weaviate(Query):\n",
    "    client = Utility.registerClient()    \n",
    "    retriever = client.collections.get(collection)\n",
    "    \n",
    "\n",
    "    response = retriever.query.bm25( # search without model\n",
    "        # query=\"Conclusion\",\n",
    "        query = Query,\n",
    "        limit=1,\n",
    "        query_properties=[\"content\"],\n",
    "        return_metadata=MetadataQuery(score=True),\n",
    "    )\n",
    "    client.close()  # Free up resources\n",
    "    for obj in response.objects:\n",
    "        result =  {\n",
    "            \"Title\": {obj.properties['title']},\n",
    "            \"Content\": obj.properties['content']\n",
    "        }\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "# # search 2\n",
    "# def search_weaviate(Query):\n",
    "#     client = Utility.registerClient()    \n",
    "#     retriever = client.collections.get(collection)\n",
    "\n",
    "#     # find the right doc title\n",
    "#     response_doc_title = retriever.query.fetch_objects(\n",
    "#         filters=Filter.by_property(\"source_type\").equal(\"summary\"),\n",
    "#         limit=500\n",
    "#     )\n",
    "#     docs = []\n",
    "#     for obj in response_doc_title.objects:\n",
    "#         content = obj.properties['content'].replace('\\n\\n', '')\n",
    "#         docs.append(f\"Title: {obj.properties['title']}, Summary: {content}\")\n",
    "    \n",
    "#     ranked_docs = get_highest_score_sim(docs)\n",
    "#     doc_title = ranked_docs[0][1].split(\",\")[0].split(\"Title: \")[-1]\n",
    "    \n",
    "#     # search on specify doc_title\n",
    "#     response = retriever.query.bm25( # search without model\n",
    "#         query = Query,\n",
    "#         limit=3,\n",
    "#         query_properties=[\"content\"],\n",
    "#         filters=Filter.by_property(\"title\").equal(doc_title),\n",
    "#         return_metadata=MetadataQuery(score=True),\n",
    "#     )\n",
    "#     client.close()  # Free up resources\n",
    "#     result = {}\n",
    "#     for obj in response.objects:\n",
    "#         if \"Title\" in result:\n",
    "#             result[\"Content\"] += \"\\n\\n\" + obj.properties['content']\n",
    "#         else:\n",
    "#             result[\"Title\"] = obj.properties['title']\n",
    "#             result[\"Content\"] = obj.properties['content']\n",
    "        \n",
    "#     return result\n",
    "\n",
    "\n",
    "print(search_weaviate(question))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f071379d-7131-41a0-b5d8-2d1780a39843",
   "metadata": {},
   "source": [
    "## Wikipedia free api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "01aa3e97-9c6e-4002-a0b7-ac70a7b130b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No search results found.\n"
     ]
    }
   ],
   "source": [
    "def search_wikipedia(query):\n",
    "    # Step 1: Search for the most relevant page\n",
    "    search_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    search_params = {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"search\",\n",
    "        \"srsearch\": query,\n",
    "        \"format\": \"json\",\n",
    "        \"srlimit\": 50 \n",
    "    }\n",
    "    search_response = requests.get(search_url, params=search_params)\n",
    "    search_data = search_response.json()\n",
    "\n",
    "    if not search_data[\"query\"][\"search\"]:\n",
    "        return \"No search results found.\"\n",
    "\n",
    "    # Step 2: Get the title of the highest match with questions\n",
    "    # use snippet to filter web page\n",
    "    ###############################################################\n",
    "    list_of_snippets = [f\"Title: {i['title']}, Snippet: {i['snippet']}\" for i in search_data[\"query\"][\"search\"]] # nested list\n",
    "    ranked_docs = get_highest_score_sim(list_of_snippets)\n",
    "\n",
    "\n",
    "    page_title = ranked_docs[0][1].split(\",\")[0].split(\"Title: \")[-1]\n",
    "    # Step 3: Fetch the extract/content of that page\n",
    "    content_params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "        \"titles\": page_title,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    content_response = requests.get(search_url, params=content_params)\n",
    "    content_data = content_response.json()\n",
    "\n",
    "    pages = content_data[\"query\"][\"pages\"]\n",
    "    for page_id, page in pages.items():\n",
    "        result =  {\n",
    "            \"Title\": {page_title},\n",
    "            \"Content\": page.get(\"extract\", \"No content found.\")\n",
    "        }\n",
    "\n",
    "        return result\n",
    "\n",
    "# Example usage\n",
    "# print(search_wikipedia(question))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9351014d-d5b1-4ca9-91b1-7e4b702d1e44",
   "metadata": {},
   "source": [
    "## Check Relevant Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "176b4553-4806-443b-a383-57162b897b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere import ChatCohere\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "def format_docs(docs): # result in long text type str\n",
    "    # return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    return RunnableLambda(lambda _: \"\\n\\n\".join(doc for doc in docs))\n",
    "\n",
    "\n",
    "\n",
    "# Question\n",
    "# result = rag_chain.invoke(question)\n",
    "# print(f\"\\nFianl Answer: {result}\")\n",
    "# print(result.split(\"Answer:\")[-1].strip().split()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "718a7014-2c97-476e-b8a5-64d424e1a7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    documents: List[str]\n",
    "    grade: str\n",
    "    generation: str\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ebc45fb9-4ffe-45fa-af66-1d1b1352dabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "import re\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = search_wikipedia(question)\n",
    "    # web_results = Document(page_content=docs['Content'])\n",
    "\n",
    "    return {\"documents\": docs['Content'], \"question\": question}\n",
    "    \n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = search_weaviate(question)\n",
    "    return {\"documents\": documents['Content'], \"question\": question}\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved document are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates document key with only relevant document\n",
    "    \"\"\"\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are a grader assessing relevance of a retrieved document to a user question.\n",
    "\n",
    "        If the document contains keyword(s) or semantic meaning related to the question, grade it as **relevant**.\n",
    "\n",
    "        ONLY return a single word: **yes** or **no** — do not explain your reasoning.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question:\n",
    "        {question}\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # LLM\n",
    "    llm = OllamaLLM(\n",
    "        model=\"deepseek-r1:7b\",\n",
    "        temperature=0,\n",
    "        base_url=\"http://host.docker.internal:11434\" \n",
    "    )\n",
    "    \n",
    "    # Chain\n",
    "    rag_chain = (\n",
    "        {\"context\": format_docs([documents]), \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        result = rag_chain.invoke(question)\n",
    "        print(f\"old format : {result.split('Answer:')[-1].strip().split()[0].lower()}\")\n",
    "    \n",
    "        # Extract yes or no\n",
    "        match = re.search(r'\\b(yes|no)\\b', result.strip().lower())\n",
    "        grade = match.group(1) if match else \"no\"\n",
    "    except:\n",
    "        match = re.search(r'\\b(yes|no)\\b', result.strip().lower())\n",
    "        grade = match.group(1) if match else \"no\"\n",
    "\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    print(f\"Grade result: {grade}\")\n",
    "    print(\"###################################################################\")\n",
    "    print(result)\n",
    "    print(\"###################################################################\")\n",
    "\n",
    "    return {\"documents\": documents, \"question\": question, \"grade\": grade }\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    print(\"---DECIDE TO GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "    grade = state[\"grade\"].lower()\n",
    "    print(f\"Grade result: {grade}\")\n",
    "\n",
    "    if grade == 'no':\n",
    "        print(\"---DECISION: DOCUMENT ARE NOT RELEVANT TO QUESTION, WEB SEARCH---\")\n",
    "        return \"web_search\"\n",
    "    else:\n",
    "        print(\"---DECISION: DOCUMENT ARE RELEVANT GO CHECK HALLUCINATION---\")\n",
    "        return \"check_hallucinate\"\n",
    "\n",
    "def check_hallucinate(state):\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    prompt = PromptTemplate.from_template(\n",
    "       \"\"\"\n",
    "       You are a grader assessing whether an LLM generation is grounded in and supported by a set of retrieved facts.\n",
    "\n",
    "       ONLY respond with a single word: **yes** or **no**.\n",
    "       - \"yes\" = the generation is grounded in the context.\n",
    "       - \"no\" = the generation is not grounded or is hallucinated.\n",
    "\n",
    "       Context:\n",
    "       {context}\n",
    "\n",
    "       Question:\n",
    "       {question}\n",
    "\n",
    "       Answer:\n",
    "       \"\"\"\n",
    "    )\n",
    "    \n",
    "    # LLM\n",
    "    llm = OllamaLLM(\n",
    "        model=\"deepseek-r1:7b\",\n",
    "        temperature=0,\n",
    "        base_url=\"http://host.docker.internal:11434\" \n",
    "    )\n",
    "    \n",
    "    # Chain\n",
    "    rag_chain = (\n",
    "        {\"context\": format_docs([documents]), \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    result = rag_chain.invoke(question)\n",
    "\n",
    "    try:\n",
    "        result = rag_chain.invoke(question)\n",
    "        print(f\"old format : {result.split('Answer:')[-1].strip().split()[0].lower()}\")\n",
    "    \n",
    "        # Extract yes or no\n",
    "        match = re.search(r'\\b(yes|no)\\b', result.strip().lower())\n",
    "        grade = match.group(1) if match else \"no\"\n",
    "    except:\n",
    "        match = re.search(r'\\b(yes|no)\\b', result.strip().lower())\n",
    "        grade = match.group(1) if match else \"no\"\n",
    "        \n",
    "    print(\"---CHECKING HALLUCINATION---\")\n",
    "    print(f\"Grade result: {grade}\")\n",
    "    print(\"###################################################################\")\n",
    "    print(result)\n",
    "    print(\"###################################################################\")\n",
    "\n",
    "    return {\"documents\": documents, \"question\": question, \"grade\": grade }\n",
    "        \n",
    "def decide_final_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"---DECIDE FINAL GENERATION---\")\n",
    "    question = state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "    grade = state[\"grade\"].lower()\n",
    "    print(f\"Grade result: {grade}\")\n",
    "    \n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "        return \"useful\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not useful\"\n",
    "        \n",
    "def final_generate(state):\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are a helpful AI assistant. Based on the provided context, generate a concise and accurate answer to the question.\n",
    "        \n",
    "        Context:\n",
    "        {context}\n",
    "    \n",
    "        Question:\n",
    "        {question}\n",
    "    \n",
    "        Answer (based only on the context above):\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # LLM\n",
    "    llm = OllamaLLM(\n",
    "        model=\"deepseek-r1:7b\",\n",
    "        temperature=0,\n",
    "        base_url=\"http://host.docker.internal:11434\" \n",
    "    )\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    # Chain\n",
    "    rag_chain = (\n",
    "        {\"context\": format_docs([documents]), \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    print(\"---FINAL GENERATE---\")\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke(question)\n",
    "    print(f\"FINAL ANSWER: {generation}\")\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "    \n",
    "\n",
    "        \n",
    "def llm_fallback(state):\n",
    "    \"\"\"\n",
    "    Generate answer using the LLM w/o vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---LLM Fallback---\")\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    prompt = PromptTemplate.from_template(\n",
    "       \"\"\"You are an assistant for question-answering tasks. Answer the question based upon your knowledge. Use three sentences maximum and keep the answer concise.\"\"\"\n",
    "    )\n",
    "    \n",
    "    # LLM\n",
    "    llm = OllamaLLM(\n",
    "        model=\"deepseek-r1:7b\",\n",
    "        temperature=0,\n",
    "        base_url=\"http://host.docker.internal:11434\" \n",
    "    )\n",
    "    \n",
    "    # Chain\n",
    "    rag_chain = (\n",
    "        {\"question\": RunnablePassthrough()}  # Add this step\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    generation = rag_chain.invoke({\"question\": question})\n",
    "    \n",
    "    return {\"question\": question, \"generation\": generation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "27cbec36-f0d3-48aa-a700-0e1f1ed80a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"check_hallucinate\", check_hallucinate)  # check_hallucinate\n",
    "workflow.add_node(\"web_search\", web_search)  # web_search\n",
    "workflow.add_node(\"llm_fallback\", llm_fallback)  # llm_fallback\n",
    "workflow.add_node(\"final_generate\", final_generate)  # final_generate\n",
    "\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_edge(\"web_search\", \"grade_documents\")\n",
    "\n",
    "# Add conditional edges from grade_documents\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"check_hallucinate\": \"check_hallucinate\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Add conditional edges from check_hallucinate\n",
    "workflow.add_conditional_edges(\n",
    "    \"check_hallucinate\",\n",
    "    decide_final_generate,\n",
    "    {\n",
    "        \"not useful\": \"web_search\",\n",
    "        \"useful\": \"final_generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"final_generate\", END)\n",
    "\n",
    "# Optionally handle fallback path\n",
    "workflow.add_edge(\"llm_fallback\", END)\n",
    "    \n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "17d5d4f6-b9fb-492b-9b27-d6f260132473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "old format : <think>\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "Grade result: yes\n",
      "###################################################################\n",
      "<think>\n",
      "Okay, so I need to determine if the document is relevant to the user's question. The context given includes several pieces of information: musduc, nan, 10, Cairina moschata, Muscovy Duck, 2023, and 423. \n",
      "\n",
      "The user's question is asking for a species_code where the scientific_name is equal to Cairina moschata, specifically looking for the latest taxonomic_sort_order. So they want the most recent one.\n",
      "\n",
      "Looking at the context, I see that Cairina moschata is mentioned as Muscovy Duck. The numbers 2023 and 423 are present. It's possible that these numbers represent years or identifiers related to species data. Since the user is asking for the latest taxonomic_sort_order, which likely refers to the most recent version or update number, the presence of 423 might be the identifier they're looking for.\n",
      "\n",
      "Therefore, the document does contain relevant information because it includes the scientific name and a possible identifier (423) that could correspond to the latest taxonomic sort order. So I should grade this as \"yes\".\n",
      "</think>\n",
      "\n",
      "yes\n",
      "###################################################################\n",
      "---DECIDE TO GENERATE---\n",
      "Grade result: yes\n",
      "---DECISION: DOCUMENT ARE RELEVANT GO CHECK HALLUCINATION---\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "old format : <think>\n",
      "---CHECKING HALLUCINATION---\n",
      "Grade result: yes\n",
      "###################################################################\n",
      "<think>\n",
      "Okay, so I need to figure out whether the answer given is grounded in the provided context. Let's break this down step by step.\n",
      "\n",
      "First, let's look at the context: musduc | nan | 10 | Cairina moschata | Muscovy Duck | 2023 | 423\n",
      "\n",
      "I think each of these values corresponds to specific fields. Maybe it's something like species code, taxonomic order, scientific name, etc. Let me try to parse this.\n",
      "\n",
      "The first value is musduc. That might be a species code or an identifier. The second is nan, which could stand for something else—maybe not relevant here. Third is 10, perhaps the taxonomic rank like family or order? Then there's Cairina moschata, which I recognize as the scientific name of the Muscovy Duck. Next is Muscovy Duck in English, then 2023 and 423.\n",
      "\n",
      "Now, the question is asking for a species_code that has a scientific_name equal to Cairina moschata and the latest taxonomic_sort_order. So they want the most recent or highest taxonomic rank available.\n",
      "\n",
      "Looking at the context, the species code might be musduc, but I'm not sure what it stands for. The taxonomic_sort_order is probably 10, which could mean a specific order in the taxonomy hierarchy. Since Muscovy Duck is a type of duck, its taxonomic classification would typically include more detailed ranks like infrakingdom, kingdom, phylum, class, order, family, genus, species.\n",
      "\n",
      "In this context, the scientific name is given as Cairina moschata, which is the species level (genus and species). The taxonomic_sort_order in the data is 10. I'm not exactly sure what 10 represents here—maybe it's a specific order like \"Order\" or another classification.\n",
      "\n",
      "The question wants the latest taxonomic_sort_order, so perhaps they're looking for the highest rank available. If the context only goes down to species (which is genus + species), then maybe 10 is the correct answer because that would be the most recent or detailed level provided.\n",
      "\n",
      "Wait, but in biological taxonomy, the order is a higher rank than family. So if Muscovy Duck is at the order level, perhaps the taxonomic_sort_order of 10 refers to the order. But I'm not entirely certain about this mapping without more context.\n",
      "\n",
      "Alternatively, maybe musduc is the species code, and they want that with the scientific name matching. If the answer given was musduc, then it's correct because it matches the scientific name Cairina moschata as per the data provided.\n",
      "\n",
      "But since the question also mentions \"latest taxonomic_sort_order,\" I think they're looking for the highest rank available in the data. So if 10 is an order and that's the most specific level given, then using 10 would be correct.\n",
      "\n",
      "Putting it all together: The species_code (musduc) corresponds to the scientific name Cairina moschata, which is Muscovy Duck. The taxonomic_sort_order of 10 seems appropriate as the latest or highest rank provided in the context. Therefore, if the answer was musduc with a sort order of 10, it would be grounded.\n",
      "</think>\n",
      "\n",
      "yes\n",
      "###################################################################\n",
      "---DECIDE FINAL GENERATION---\n",
      "Grade result: yes\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\"Node 'check_hallucinate':\"\n",
      "'\\n---\\n'\n",
      "---FINAL GENERATE---\n",
      "FINAL ANSWER: <think>\n",
      "Okay, so I need to figure out the species code for a specific scientific name. The question is asking for a species_code where the scientific_name is \"Cairina moschata\" and it should be the latest taxonomic_sort_order as of 2023.\n",
      "\n",
      "Looking at the context provided: musduc | nan | 10 | Cairina moschata | Muscovy Duck | 2023 | 423\n",
      "\n",
      "I see that \"Cairina moschata\" is paired with the number 423. The other fields are musduc, nan, and 10, but I'm not sure what those represent. Since the question is about species_code, which likely refers to a code assigned by the International Union for Conservation of Nature (IUCN), I think that's where the 423 comes from.\n",
      "\n",
      "The taxonomic_sort_order probably refers to the order in which species are classified. The latest one would be the most recent or highest rank. Since 423 is associated with Muscovy Duck and it's listed as of 2023, I can assume that this code is up-to-date.\n",
      "\n",
      "So putting it all together, the species_code for Cairina moschata in the latest taxonomic order from 2023 is 423.\n",
      "</think>\n",
      "\n",
      "The species code for \"Cairina moschata\" as of the latest taxonomic_sort_order (2023) is **423**. \n",
      "\n",
      "Answer: 423\n",
      "\"Node 'final_generate':\"\n",
      "'\\n---\\n'\n",
      "('<think>\\n'\n",
      " 'Okay, so I need to figure out the species code for a specific scientific '\n",
      " 'name. The question is asking for a species_code where the scientific_name is '\n",
      " '\"Cairina moschata\" and it should be the latest taxonomic_sort_order as of '\n",
      " '2023.\\n'\n",
      " '\\n'\n",
      " 'Looking at the context provided: musduc | nan | 10 | Cairina moschata | '\n",
      " 'Muscovy Duck | 2023 | 423\\n'\n",
      " '\\n'\n",
      " 'I see that \"Cairina moschata\" is paired with the number 423. The other '\n",
      " \"fields are musduc, nan, and 10, but I'm not sure what those represent. Since \"\n",
      " 'the question is about species_code, which likely refers to a code assigned '\n",
      " \"by the International Union for Conservation of Nature (IUCN), I think that's \"\n",
      " 'where the 423 comes from.\\n'\n",
      " '\\n'\n",
      " 'The taxonomic_sort_order probably refers to the order in which species are '\n",
      " 'classified. The latest one would be the most recent or highest rank. Since '\n",
      " \"423 is associated with Muscovy Duck and it's listed as of 2023, I can assume \"\n",
      " 'that this code is up-to-date.\\n'\n",
      " '\\n'\n",
      " 'So putting it all together, the species_code for Cairina moschata in the '\n",
      " 'latest taxonomic order from 2023 is 423.\\n'\n",
      " '</think>\\n'\n",
      " '\\n'\n",
      " 'The species code for \"Cairina moschata\" as of the latest '\n",
      " 'taxonomic_sort_order (2023) is **423**. \\n'\n",
      " '\\n'\n",
      " 'Answer: 423')\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\"question\": question}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint.pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "    pprint.pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint.pprint(str(value[\"generation\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "022c29d1-b3c2-4c80-abf3-b3086ff5f8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "musduc | nan | 10 | Cairina moschata | Muscovy Duck | 2023 | 423\n",
      "\n",
      "\n",
      "\n",
      "give me a species_code that have scientific_name equal to Cairina moschata and leatest taxonomic_sort_order\n",
      "\n",
      "\n",
      "\n",
      "<think>\n",
      "Okay, so I need to figure out the species code for a specific scientific name. The question is asking for a species_code where the scientific_name is \"Cairina moschata\" and it should be the latest taxonomic_sort_order as of 2023.\n",
      "\n",
      "Looking at the context provided: musduc | nan | 10 | Cairina moschata | Muscovy Duck | 2023 | 423\n",
      "\n",
      "I see that \"Cairina moschata\" is paired with the number 423. The other fields are musduc, nan, and 10, but I'm not sure what those represent. Since the question is about species_code, which likely refers to a code assigned by the International Union for Conservation of Nature (IUCN), I think that's where the 423 comes from.\n",
      "\n",
      "The taxonomic_sort_order probably refers to the order in which species are classified. The latest one would be the most recent or highest rank. Since 423 is associated with Muscovy Duck and it's listed as of 2023, I can assume that this code is up-to-date.\n",
      "\n",
      "So putting it all together, the species_code for Cairina moschata in the latest taxonomic order from 2023 is 423.\n",
      "</think>\n",
      "\n",
      "The species code for \"Cairina moschata\" as of the latest taxonomic_sort_order (2023) is **423**. \n",
      "\n",
      "Answer: 423\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for valus in output.values():\n",
    "    print(v['documents'])\n",
    "    print('\\n\\n')\n",
    "    print(v['question'])\n",
    "    print('\\n\\n')\n",
    "    print(v['generation'])\n",
    "    print('\\n\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
