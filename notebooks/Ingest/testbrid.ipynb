{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d827a92-d03c-4e5e-8dbd-92bd9f7f5129",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# paramter cell do not remove!!\n",
    "nb_parm=''\n",
    "question = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "575c082c-c36d-49c2-a58f-f477e2635af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"/home/jovyan/notebooks\")\n",
    "from Framework.module import Utility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3948dd-3761-4bdd-b62c-e8eb094ea50b",
   "metadata": {},
   "source": [
    "## Do the task After this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6d7aea-d442-400e-9dd3-104a3c7597f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb50f66b-425e-4436-a0e1-622dd2a1a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Since Ollama and Jupyter Notebook run in separate containers, \n",
    "Jupyter needs to communicate with Ollama via HTTP requests, \n",
    "because they are not in the same process or environment.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1910727-4204-4506-b629-83412dbf89ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "The capital of Thailand is Bangkok.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "# Explicitly define Ollama's API base URL\n",
    "llm = OllamaLLM(\n",
    "    model=\"deepseek-r1:7b\",\n",
    "    temperature=0,\n",
    "    # base_url=\"http://ollama:11434\"\n",
    "    base_url=\"http://host.docker.internal:11434\" # need to refer because it run as separate container the langchain not know what the end point of model are so this is need to be attached\n",
    "\n",
    ")\n",
    "\n",
    "# Define a simple question prompt\n",
    "question = \"What is the capital of thailand?\"\n",
    "response = llm.invoke(question)\n",
    "\n",
    "# Print the response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1f1a94a-9031-4b63-b57c-f5f44712c9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the collection\n",
    "client.collections.delete(\"Question\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8d83b86-ee99-4219-bd00-2be6c76f0248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/weaviate/collections/classes/config.py:2082: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  for cls_field in self.model_fields:\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "from weaviate.classes.config import Configure\n",
    "\n",
    "client = weaviate.connect_to_custom(\n",
    "    http_host=\"host.docker.internal\",\n",
    "    http_port=8081,\n",
    "    http_secure=False,  # Set to True if using HTTPS\n",
    "    grpc_host=\"host.docker.internal\",\n",
    "    grpc_port=50051,\n",
    "    grpc_secure=False,  # Set to True if using a secure gRPC connection\n",
    ")\n",
    "\n",
    "print(client.is_ready())  # Should return True if the connection is successful\n",
    "\n",
    "questions = client.collections.create(\n",
    "    name=\"Question\",\n",
    "    vectorizer_config = Configure.Vectorizer.text2vec_ollama(\n",
    "    api_endpoint=\"http://host.docker.internal:11434\",  # Ensures Weaviate inside Docker can talk to Ollama on host\n",
    "    model=\"mxbai-embed-large\"                          # The Ollama model name for embedding nomic-embed-text, mxbai-embed-large\n",
    "    ),\n",
    "    generative_config=Configure.Generative.ollama(              # Configure the Ollama generative integration\n",
    "        api_endpoint=\"http://host.docker.internal:11434\",       # Allow Weaviate from within a Docker container to contact your Ollama instance\n",
    "        model=\"deepseek-r1:7b\",                                       # The model to use\n",
    "    )\n",
    ")\n",
    "\n",
    "client.close()  # Free up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebf6df07-57d2-47f8-8f5f-2dd572b31439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "import requests, json\n",
    "\n",
    "client = weaviate.connect_to_custom(\n",
    "    http_host=\"host.docker.internal\",\n",
    "    http_port=8081,\n",
    "    http_secure=False,  # Set to True if using HTTPS\n",
    "    grpc_host=\"host.docker.internal\",\n",
    "    grpc_port=50051,\n",
    "    grpc_secure=False,  # Set to True if using a secure gRPC connection\n",
    ")\n",
    "resp = requests.get(\n",
    "    \"https://raw.githubusercontent.com/weaviate-tutorials/quickstart/main/data/jeopardy_tiny.json\"\n",
    ")\n",
    "data = json.loads(resp.text)\n",
    "\n",
    "questions = client.collections.get(\"Question\")\n",
    "\n",
    "with questions.batch.dynamic() as batch:\n",
    "    for d in data:\n",
    "        batch.add_object({\n",
    "            \"answer\": d[\"Answer\"],\n",
    "            \"question\": d[\"Question\"],\n",
    "            \"category\": d[\"Category\"],\n",
    "        })\n",
    "        if batch.number_errors > 10:\n",
    "            print(\"Batch import stopped due to excessive errors.\")\n",
    "            break\n",
    "\n",
    "failed_objects = questions.batch.failed_objects\n",
    "if failed_objects:\n",
    "    print(f\"Number of failed imports: {len(failed_objects)}\")\n",
    "    print(f\"First failed object: {failed_objects[0]}\")\n",
    "\n",
    "client.close()  # Free up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ce9a906-8cee-4cb5-9374-ea5f44cc39cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"answer\": \"DNA\",\n",
      "  \"question\": \"In 1953 Watson & Crick built a model of the molecular structure of this, the gene-carrying substance\",\n",
      "  \"category\": \"SCIENCE\"\n",
      "}\n",
      "{\n",
      "  \"answer\": \"Liver\",\n",
      "  \"question\": \"This organ removes excess glucose from the blood & stores it as glycogen\",\n",
      "  \"category\": \"SCIENCE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "import json\n",
    "\n",
    "client = weaviate.connect_to_custom(\n",
    "    http_host=\"host.docker.internal\",\n",
    "    http_port=8081,\n",
    "    http_secure=False,  # Set to True if using HTTPS\n",
    "    grpc_host=\"host.docker.internal\",\n",
    "    grpc_port=50051,\n",
    "    grpc_secure=False,  # Set to True if using a secure gRPC connection\n",
    ")\n",
    "\n",
    "questions = client.collections.get(\"Question\")\n",
    "\n",
    "response = questions.query.near_text(\n",
    "    query=\"biology\",\n",
    "    limit=2\n",
    ")\n",
    "\n",
    "for obj in response.objects:\n",
    "    print(json.dumps(obj.properties, indent=2))\n",
    "\n",
    "client.close()  # Free up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c7c9bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/langsmith/client.py:278: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<think>\\nOkay, so I need to figure out what Task Decomposition is based on the given context. Let me read through the context carefully.\\n\\nThe context mentions that complicated tasks usually involve many steps, and agents need to plan ahead. Then there's a section about Task Decomposition, which talks about Chain of Thought (CoT) as a prompting technique used by large language models (LLMs). CoT helps break down complex tasks into smaller, manageable parts and provides insights into the model's thinking process.\\n\\nSo putting that together, Task Decomposition refers to the process of breaking down complex tasks into simpler steps. This allows agents or models to handle each part more effectively. The context specifically mentions using Chain of Thought as a method for this decomposition, which enhances performance by utilizing more computational resources during execution.\\n</think>\\n\\nTask Decomposition is the process of breaking down complex tasks into smaller, manageable steps to enhance problem-solving and efficiency. By decomposing tasks, agents or models can handle each component more effectively, improving overall performance through step-by-step processing guided by Chain of Thought techniques.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_community.vectorstores import Weaviate\n",
    "import weaviate\n",
    "\n",
    "#### INDEXING ####\n",
    "\n",
    "# Load Documents\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load() # list of str\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs) # each element is <class 'langchain_core.documents.base.Document'>\n",
    " \n",
    "\n",
    "# Embed\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OllamaEmbeddings(\n",
    "                                                model=\"mxbai-embed-large\",  # Or another embedding model, like `nomic-embed-text`\n",
    "                                                base_url=\"http://host.docker.internal:11434\"\n",
    "                                            )\n",
    "                                   )\n",
    "\n",
    "\n",
    "# vectorstore = Weaviate(\n",
    "#     client = Utility.registerClient(),\n",
    "#     index_name=\"BridsKnowledge\",  # Your collection/class name in Weaviate\n",
    "#     text_key=\"content\",  # The field that holds the actual chunk of text\n",
    "#     embedding=OllamaEmbeddings(\n",
    "#         model=\"mxbai-embed-large\",\n",
    "#         base_url=\"http://host.docker.internal:11434\"\n",
    "#     )\n",
    "# )\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "## Weaviate retrieval That’s when it sends a query to Weaviate, uses the content field for semantic search, and returns the most relevant chunks based on your query.\n",
    "# docs = retriever.get_relevant_documents(\"your query here\")\n",
    "\n",
    "### RETRIEVAL and GENERATION ####\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM\n",
    "llm = llm = OllamaLLM(\n",
    "    model=\"deepseek-r1:7b\",\n",
    "    temperature=0,\n",
    "    base_url=\"http://host.docker.internal:11434\" \n",
    ")\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs): # result in long text type str\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"Summary this web page for futher retrieval purpose\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
